<h1>Hash tables</h1>
<p>
Suppose we want a data structure to implement either a <strong>mutable
set</strong> of elements with operations like <code>contains</code>,
<code>add</code>, and <code>remove</code> that take an element
as an argument, or a <strong>mutable map</strong> from keys to
values with operations like <code>get</code>, <code>put</code>, and
<code>remove</code> that take a key or a (key,value) pair for an
argument. A <strong>map</strong> represents a partial function that maps
keys to values; it is <strong>partial</strong> because it is not defined
on all keys, but only on those that have been put into the map. We have
now seen a few data structures that could be used for both of these
implementation tasks.
</p>
<p>
We consider the problem of implementing sets and maps together, because
most data structures that can implement a set can also implement a
map and vice versa. A set of (key,value) pairs can act as a map,
provided there is at most one (key,value) pair in the map with a given
key. Conversely, a map that maps every key to a fixed dummy element can
be used to represent a set of keys.
</p>
<p>
Here are the data structures we've seen so far, with the asymptotic complexities
for each of their main operations:
</p>
<center>
<table class=rowlines>
<tr>
<th>Data structure</th> <th>lookup (contains/get)</th> <th>add/put</th> <th>remove</th></tr>
<tr>
<td>Array</td>     <td>\(O(n)\)</td>         <td>\(O(1)\)</td>  <td>\(O(n)\)</td>
</tr><tr>
<td>Sorted array</td>  <td>\(O(\lg n)\)</td>        <td>\(O(n)\)</td>  <td>\(O(n)\)</td>
</tr><tr>
<td>Linked list</td>  <td>\(O(n)\)</td>         <td>\(O(1)\)</td>  <td>\(O(n)\)</td>
</tr><tr>
<td>Balanced search tree</td>  <td>\(O(\lg n)\)</td>        <td>\(O(\lg n)\)</td> <td>\(O(\lg n)\)</td>
</tr>
</table>
</center>
<p>
Naturally, we might wonder if there is a data structure that can do better.
And it turns out that there is: the <strong>hash table</strong>, one of the best and
most useful data structures—when used correctly.
</p>
<p>
Many variations on hash tables have been developed. We'll explore the most
common ones, building up in steps.
</p>
<h2>Step 1: Direct address tables</h2>
<p>
While arrays are a slow implementation of maps and sets when you don't know the right index to
look at, all of their operations are very fast when you do. This insight underlies
the <strong>direct address table</strong>. Suppose that for each element that
we want to store in the data structure, we can determine a unique integer index
in the range \(0\dots m-1\). That is, we need an <strong>injective</strong> function that maps
elements (or keys) to integers in the range. Then we can use the indices produced
by the function to decide at which index to store the elements in an array of size \(m\).
</p>
<div class="floatfigure">
<canvas id="dat-figure" style="width:350px"></canvas>
<script class=graphics>
with (new CFigure("dat-figure")) {
    const arr = rectangle().setW(55).setFillStyle(null), m = margin()
    align("none", "TB", arr, m)
    const key = label("key: 29 Pine St.")
    const cell29 = rectangle(), cell34 = rectangle()
    const lb28 = label("28")
    sameSize(cell29, cell34)
    const obj = varBoxes("=House", "address=29", "street=Pine", "color=blue")
    const obj2 = varBoxes("=House", "address=34", "street=Pine", "color=white")
    align("LR", "none", cell29, arr, cell34)
    arrow(key.toRight(5), cell29.toLeft(20))
    align("distribute", "none", key, arr, obj.getVar(0))
    align("none", "TB", cell29, obj.getVar(0))
    align("none", "TB", cell34, obj2.getVar(0))
    align("left", "none", key, m)
    align("right", "none", obj, m)
    align("none", "center", key, cell29)
    align("left", "distribute", lb28,
        group(label("29"), hspace(4).setH(0), cell29).align("abut", "center"),
        label("30"),
        label("31"),
        label("32"),
        label("33"),
        group(label("34"), hspace(4).setH(0), cell34).align("abut", "center"),
        label("35")
    )
    pointer(cell29, obj.getVar(0))
    pointer(cell34, obj2.getVar(0))
    align("LR", "none", obj, obj2)
    align("none", "top", lb28, arr)
    geq(obj2.y0(), plus(obj.y1(), 10))
    align("R", "B", obj2, m)
}
</script>
</div>
<p>
For example, suppose we are maintaining a collection of objects representing
houses on the same street. We can use the street address as the index into a
direct address table. For example, to find the house at 29 Pine St., we might
use the index 29, as shown in the figure on the right. Not every possible
street address will be used, so some array entries will be empty.
Empty entries are not a problem unless there are too many of them.
</p>
<p>
However, it is often hard to come up with an injective function that
does not require many empty entries. For example, we might try to
maintain a collection of employees along with their social security
numbers, and to look them up by social security number. Using the social
security number as the index into a direct address table would require
an array of 1 billion elements, almost all of which are likely to be
unused. Even assuming our computer has enough memory to store such a
<strong>sparse</strong> array (probably at least 8 GiB), it will be a waste of memory. Furthermore, on
most computer hardware, the use of caches means that accesses to large
arrays are actually significantly slower than accesses to small arrays,
sometimes by two orders of magnitude.
</p>

<h2>Step 2: Hashing</h2>
<div class="floatfigure">
<canvas id="hashfn" style="width: 250px; height: 200px"></canvas>
<script class=graphics>
function hashfn(f, txt) {
    const grd = f.ctx.createLinearGradient(0, 0, 0, 80)
    grd.addColorStop(0, "#cff")
    grd.addColorStop(1, "#ccc")
    const outer = f.box()
    const p0 = outer.ul(), p1 = outer.ur(),
        p2 = f.point(f.variable("p2x"), outer.y1()),
        p3 = f.point(f.variable("p3x"), outer.y1())
    f.equal(f.minus(p2.x(), p0.x()), f.minus(p1.x(), p3.x()))
    f.equal(f.minus(p1.x(), p0.x()), f.times(1.5, f.minus(p3.x(), p2.x())))
    f.equal(f.minus(p3.x(), p2.x()), f.minus(p2.y(), p0.y()))
    const p = f.polygon(p0, p1, p3, p2).setCornerRadius(10).setFillStyle(grd)
        .addText(txt)
    return p
}
function small_word(fig, i, nolabel) {
    const result = fig.rectangle().setW(45).setH(15)
    if (!nolabel) fig.label(`${i}`).at(result.cl().toLeft(10)).setFontSize(10)
    return result
}
function small_words(fig, n, nolabel) {
    const arr = []
    for (let i = 0; i < n; i++) arr[i] = small_word(fig, i, nolabel)
    return arr
}
const hashfn_fig = new CFigure("hashfn")
with (hashfn_fig) {
    const arr = small_words(hashfn_fig, 13)
    const m = margin()
    const g = group(...arr).align("LR", "abut")
    const lb10 = label("130000010")
    const ssn = label("452321776")
    const h = hashfn(hashfn_fig,"hash\nfunction:\nkey % 13").setW(90).setH(60)
    align("center", "none", average(lb10, ssn), h)
    align("right", "top", g, m)
    align("left", "top", lb10, m)
    align("abut", "top", lb10, hspace(30), ssn)
    setConnectionStyle("intersection")
    arrow(lb10, h).setLineDash([4,4])
    arrow(ssn, h).setLineDash([4,4])
    const p4 = point(h.x(), arr[10].y())
    arrow(h.lc(), p4, p4, p4, arr[10].toLeft(18)).setLineDash([4,4])
}
</script>
</div>
<p>
Instead of requiring that each key be mapped to a unique index,
hash tables allow <strong>collisions</strong> in which two keys maps to the same index,
and consequently the array can be smaller, on the order of the number of
elements in the hash table. The entries in the array are called <strong>buckets</strong>,
and we use \(m\) to denote the number of buckets.
The mapping from keys to bucket indices
is performed by a <strong>hash function</strong>;
it maps the key in a reproducible way to a <strong>hash value</strong> (or just <strong>hash</strong>) that
is a legal array index.
</p>
<p>
If the hash function is good, it distributes
values fairly evenly, so that collisions occur infrequently, as if at random.
In particular, what we want from the hash function is that for any two
different keys generated by the program, the chance that they collide is
approximately \(1/m\).
</p>

<p>For example, suppose we are using an array with 13 entries and our keys are
social security numbers.  Then we might use <strong>modular hashing</strong>, in which
the array index is computed as <code>(key % 13)</code>. Modular hashing is not
very random, but is likely to be good enough in many applications.
</p>
<p>
In some applications, the keys may be generated by an adversary who
could purposely choose keys that collide. In this case, the hash
function must be chosen carefully to prevent the adversary from
engineering collisions.  Modular hashing will not be good enough. In
fact, it is possible to choose strings such that Java's built-in hashing
of strings produces collisions. When dealing with adversarial keys, a
<strong>cryptographic hash function</strong> should be used so that the
adversary cannot produce collisions at a rate higher than random choice.
</p>
<h2>Step 3: Collision resolution</h2>
<h3>Chaining</h3>
<p>
There are two main ideas for how to deal with collisions. The best way is
usually <strong>chaining</strong>: each array entry corresponds to a <strong>bucket</strong>
containing a mutable set of elements. (Confusingly, this approach is sometimes called
<strong>closed addressing</strong> or <strong>open hashing</strong>.) Typically, the bucket is
implemented as a linked list, and each nonempty array entry contains a
pointer to the head of the list. The list contains all keys that have been added to
the hash table which hash to that array index.
</p>
<p>
For example, after inserting two Employees into a hash table whose keys are social security numbers
that both map to the same bucket, we might end up with a data structure like the one depicted:
</p>
<div class=figure>
<canvas id="chaining_fig" style="width: 650px"></canvas>
<script class=graphics>
const chaining_fig = new CFigure("chaining_fig")
with (chaining_fig) {
    setConnectionStyle("intersection")
    const lb10 = label("130000010")
    const ssn = label("452321776")
    const h = hashfn(chaining_fig, "key % 13")
    const m = margin()
    const arr = small_words(chaining_fig, 13)
    const mem = group(...arr).align("LR", "abut")
    const p4 = point(h.x(), arr[10].y())
    saveStyle()
        setLineDash([4,4])
        arrow(h.lc(), p4, p4, p4, point(minus(arr[10].x0(), 18), arr[10].y()))
        arrow(lb10, h)
        arrow(ssn, h)
    restoreStyle()
    align("none", "top", lb10, ssn, margin())
    align("abut", "none", lb10, hspace(40), ssn)
    align("center", "none", average(lb10, ssn), h)
    align("none", "TB", mem, m)
    align("none", "top", arr[0], m)
    align("none", "bottom", arr[arr.length - 1], m)
    align("abut", "none", ssn, hspace(40), mem)
    align("left", "none", lb10, m)

    setFontSize(10)
    const empl1 = varBoxes("=Employee", "ssn=130000010", "salary=53k"),
          empl2 = varBoxes("=Employee", "ssn=452321776", "salary=86k")

    align("abut", "top", empl1, hspace(30), empl2)

    const node1 = varBoxes("key=130000010", "=value", "next="),
          node2 = varBoxes("key=452321776", "=value", "next=null")

setConnectionStyle("intersection")
    align("none", "bottom", node1, node2)
    pointer(node1.getVar(1), empl1)
    pointer(node2.getVar(1), empl2)
    pointer(node1.getVar(2), node2.getVar(1).cl())
    align("abut", "none", arr[10], hspace(40), node1.getVar(0))

    pointer(arr[10], node1.getVar(1).toLeft(20), node1.getVar(1))

    equal(node1.x1(), empl1.x0())
    equal(node2.x1(), empl2.x0())
    equal(node1.y0(), plus(empl1.y1(), 30))
}
</script>
<p class=caption>Hash table with chaining</p>
</div>
<p>To check whether an element is in the hash table, the key is first
hashed to find the correct bucket to look in, then the linked list is scanned
for the desired element. If the list is short, this scan is quick.
</p>
<p>
An element is added or removed by hashing it to find the correct bucket. Then,
the bucket is checked to see if the element is there, and finally the element
is added or removed appropriately from the bucket in the usual way for linked
lists.
</p>
<p>
</p>
<h3>Probing</h3>
<div class="floatfigure">
<canvas id="probing_fig" style="width: 450px"></canvas>
<script class=graphics>
const probing_fig = new CFigure("probing_fig")
with (probing_fig) {
    setConnectionStyle("intersection")
    const lb10 = label("130000010")
    const ssn = label("452321776")
    const h = hashfn(probing_fig, "key % 13")
    const m = margin()
    const arr = small_words(probing_fig, 13)
    const mem = group(...arr).align("LR", "abut")
    const p4 = point(h.x(), arr[10].y())
    saveStyle()
        setLineDash([4,4])
        arrow(h.lc(), p4, p4, p4, point(minus(arr[10].x0(), 18), arr[10].y()))
        arrow(lb10, h)
        arrow(ssn, h)
    restoreStyle()
    align("none", "top", lb10, ssn, margin())
    align("abut", "none", lb10, hspace(40), ssn)
    align("center", "none", average(lb10, ssn), h)
    align("none", "TB", mem, m)
    align("none", "top", arr[0], m)
    align("none", "bottom", arr[arr.length - 1], m)
    align("abut", "none", ssn, hspace(40), mem)
    align("left", "none", lb10, m)

    setFontSize(10)
    const empl1 = varBoxes("=Employee", "ssn=130000010", "salary=53k"),
          empl2 = varBoxes("=Employee", "ssn=452321776", "salary=86k")


    align("right", "abut", empl1, vspace(30), empl2)
    align("right", "none", empl1, m)
    pointer(arr[10], empl1.getVar(0).toLeft(50), empl1.getVar(0))
    pointer(arr[11], empl2.getVar(0).toLeft(50), empl2.getVar(0))
    setStrokeStyle("#68c")
    arrow(arr[10].cr(), arr[10].cr().toRight(30), arr[11].cr().toRight(30), arr[11].cr())
        .setLineDash([2,2])
        .setEndArrow("curved")

setConnectionStyle("intersection")
}
</script>
<p class=caption>Hash table with linear probing (stride 1)</p>
</div>
<p>
Another approach to collision resolution is
<strong>probing</strong>. (Equally confusingly, this technique is also known as
<strong>open addressing</strong> or <strong>closed hashing</strong>.) Rather
than put colliding elements in
a linked list, all elements are stored in the array itself. When adding a new
element to the hash table creates a collision, the hash table finds somewhere
else in the array to put it. The simple way to find an empty index is to search ahead
through the array indices with a fixed stride (usually 1) for the next unused array
entry, wrapping modulo the length of the array if necessary. This strategy
is called <strong>linear probing</strong>. It tends to produce a lot of
clustering of elements, leading to poor performance. A better
strategy is <strong>double hashing</strong>, in which a second hash function
is used to determine the probing interval.  Removing an element
from a hash table is not as straightforward because simply removing the array
element could mislead a probe for a different element. Instead, the element
must remain but be marked as deleted, perhaps by changing the array entry to
point to a distinguished <strong>tombstone</strong> object.
</p>

<p>Regardless of how collisions are resolved, the time required for hash
table operations grows as the hash table fills up. By contrast, the performance of
chaining degrades more gracefully, and chaining is usually faster than probing
even when the hash table is not nearly full. For these reasons, chaining is often
preferred over probing.</p>

<p>A recently popular variant of closed hashing is <strong>cuckoo
hashing</strong>, in which two hash functions are used. Each
element is stored at one of the two locations computed by these
hash functions, so at most two table locations must be consulted in
order to determine whether the element is present. If both possible
locations are occupied, the newly added element <em>displaces</em>
the element that was there, and this element is then re-added to the
table. In general, a chain of displacements occurs. In practice, it
allows a higher load factor than classic probing while avoiding long
probing sequences. </p>

<h2>Performance of hash tables</h2>

<p>Suppose we are using a chained hash table with \(m\) buckets, and the number of
elements in the hash table is \(n\). The average number of elements per bucket is
\(n/m\), which is called the <strong>load factor</strong> of the hash table, written as \(α\).  When
we search for an element that is not in the hash table, the expected length of
the linked list traversed is \(n/m = α\). If we search for an element that
<em>is</em> in the hash table, there is at least one element in the traversed
list, and the remaining \(n-1\) elements are spread across all \(m\) buckets,
so the expected length is \(1 + (n - 1)/m = 1 + α - 1/m &lt; 1+α\), though on average only
half the list needs to be traversed to find it.  Since even when \(α\) is
small, there is always at least some \(O(1)\) cost of hashing, the cost of
searching for an absent element is \(O(1+α)\), and the cost of searching for a
present element is \( O(1 + (α+1)/2) \) = \( O (1 + α) \).  Therefore, with a
good hash function all hash table operations take expected time \(O(1 + α)\).
</p>
<p>
Therefore, if we have a good hash function and we can ensure that the load factor
α never exceeds some fixed value \(α_{max}\), then all operations take \(O(1 +
α_{max}) = O(1)\) time on average.
</p>
<!--  Actually: O(lg n/lg lg n) for fixed α.
<p>
What about the time to access the worst-case element in the hash
tableα If the hash function is indistinguishable from a random function, the
probability distribution of the length of the longest linked list falls off
exponentially in the list length, so the expected longest bucket length is
still \(O(1+α)\).
</p>
-->
<p>
In practice, we get the best performance from hash tables when \(α\) is
within a narrow range, from approximately 1/2 to 1. If \(α\) is less than 1/2, the
bucket array is becoming sparse and a smaller array is likely to give better
performance. If \(α\) is greater than 1, the cost of traversing the linked lists
limits performance. However, the optimal bounds on \(α\) depend on details of
the hardware architecture.
</p>
<p>
One way to hit the desired range for \(α\) is to allocate the bucket array to just
the right size for the number of elements that are being added to it. In general,
however, it is hard to know ahead of time what this size will be, and in any case,
the number of elements in the hash table may need to change over time.
</p>

<h2>Step 4: Resizable arrays</h2>
<p>
To avoid trying to predict how big to make the bucket array ahead of time,
we can use a <strong>resizable array</strong> to dynamically adjust the size when necessary.
Instead of representing the hash table as a bucket array, we introduce a hash table
object that maintains a pointer to the current bucket array and the
number of elements that are currently in the hash table. This <strong>header object</strong>
is depicted in the figure on the right.
</p>
<div class="floatfigure">
<canvas id="resizing_fig" style="width: 250px; height:120px"></canvas>
<script class=graphics>
let resizing_fig
with (resizing_fig = new CFigure("resizing_fig")) {
    const m = margin()
    setConnectionStyle("intersection")
    const header_label = label("header object").setFontStyle("bold")
    const header = varBoxes("size=n", "buckets=")
    const arr = small_words(resizing_fig, 7, true)
    vertLine(arr[0].ur().toRight(7),
        arr[arr.length-1].lr().toRight(7)).setEndArrow("curved").setStartArrow("curved")
        .addLabel('m', 0.5, 7, 2)
    pointer(header.getVar(1), arr[0].toLeft(30), arr[0])

    group(arr).align("center", "abut")
    align("left", "abut", header_label, vspace(5), header)
    align("none", "top", arr[0], m)
    align("left", "top", header_label, m)
    align("abut", "none", header, hspace(40), arr[0])
}
</script>
</div>
<p>
If adding an element would cause \(α\) to exceed \(α_{max}\),
the hash table generates a new bucket array whose size is a multiple of
the current size, usually twice the size. This means the hash function must change,
so <em>all</em> the elements must be rehashed into the new bucket array. Hash functions
are typically designed so they take the array size \(m\) as a parameter,
so this parameter just needs to be changed.
</p>

<p>Resizable arrays are also used to implement variable-length array
abstractions like <code>ArrayList</code>, because its size can be
changed at run time.</p>

<h2>Amortized complexity</h2>

<p>With the above procedure, some <code>add</code> operations will cause
all the elements in the hash table to be rehashed. We have to search
the array for the nonempty buckets, hash all the elements, and add
them to the new table. This will take time \(O(n)\)
(provided \(n\) is at least a constant fraction of the
size of the array). For a large hash table, rehashing may take enough time
that it causes problems for programs that cannot tolerate the pause.
Perhaps surprisingly, however,
the expected cost per operation is still \(O(1)\). In
particular, any sequence of \(n\) operations on the hash table always takes
expected time \(O(n)\), or \(O(1)\)
per operation. Therefore we say that the <strong>amortized</strong> asymptotic
complexity of hash table operations is \(O(1)\). </p>
<p>
To see why this is true, consider the worst case: inserting a series of elements
into a hash table in a way that maximizes the number of rehashes. Suppose we have
a hash table with \(α_{max} = 1\). To maximize rehashes, we start
with a table of size 1, and add a sequence of \( n = 2^{j} \) elements, causing
rehashes at every power of 2, including the final add. Adding \(2^j\) elements
requires \(n = 2^j\) initial hashes. In addition,
the hash table resizes after 1 add, then again after 2 more adds, then again after 4 more adds, etc.
The cost of all the resizings is (a constant multiple of)
\(1 + 2 + 4 + 8 + \ldots + 2^{j} = 2^{j+1}&minus;1 = 2n&minus;1\). Putting the initial
hashes and the rehashes together, there are \(3n-1\) hashes in total, which is
still \(O(n)\)!
</p>
<p>
Notice that it is crucial that the array size grow geometrically (doubling).
It may be tempting to grow the array by a fixed increment (e.g., 100 elements
at time), but this causes \(n\) elements to be rehashed \(O(n)\) times on
average, resulting in \(O(n^{2})\) total insertion time, or amortized
complexity of \(O(n)\).
</p>
<p>
One way to think about this amortized analysis is that adding an element actually takes
3 times as long it seems to. Most adds only cause a single hash to be done, but then
“prepay“ for two later hashes so that the total number of hashes is never more than 3 times
the number of elements added.
</p>

<h2>Hash functions</h2>

<p>A good hash function avoids collisions. If we know
all the keys are in advance, we may be able to design a <em>perfect hash
function</em> that hashes each key to a distinct bucket. In that case,
our hash table becomes a direct address table. But in general, we
can't design a perfect hash function.
</p>
<p>
An idealized hash function
is one that satisfies the <strong>simple uniform hashing assumption</strong>: it behaves
as if it generates each bucket index randomly with equal probability whenever
it sees a new key; whenever it is given the key later, it returns the same
bucket index. Such an idealized hash function is called a <strong>random oracle</strong>.
</p>
<p>
It would be impractical to implement a hash function that behaves in exactly this way,
but we can implement a hash function that is close enough. One way to think about
the problem of designing hash functions is that there is an adversary generating
keys and trying to cause collisions. The job of the hash function designer is to
pick a hash function that defeats the adversary: that makes it unlikely that the
adversary will generate collisions. Of course, often this ”adversary“ is not
really an adversary at all; it is the rest of the program generating keys in a way that relates
to the computation being done. But some settings, there really is an adversary
influencing the choice of keys.
</p>
<p>
There are relatively inexpensive hash functions that work well for less
adversarial situations, and more expensive hash functions that make it
infeasible for an adversary to choose keys that collide. A key property we want
is <em>diffusion</em>: the hashes of two related keys should appear to have no
relation to each other.  In particular, if any two keys \(k\) and \(k'\) are
“close” to each other in the sense that the computation is likely to generate
both of them, their hashes should not have any relationship. An implication of
this principle is that if a single bit is flipped in the key, then every bit in
the hash should appear to flip with 1/2 probability.
</p>

<h2>Hash tables in the Java Collection Framework</h2>

<p>
The standard Java libraries offer multiple implementations of hash tables. The
class <code>HashSet&lt;T&gt;</code> implements a mutable set abstraction: a set
of elements of type <code>T</code>. The class <code>HashMap&lt;K,V&gt;</code>
implements a mutable map from keys of type K to values of type V. There is also
a second, older mutable map implementation, <code>Hashtable&lt;K,V&gt;</code>,
but it should be avoided; the <code>HashMap</code> class is faster and better
designed.
</p>
<div class="floatfigure">
<canvas id="hashcode_fig"></canvas>
<script class=graphics>
const hashcode_fig = new CFigure("hashcode_fig")
with (hashcode_fig) {
    const hm_bkgd = rectangle().setCornerRadius(6).setFillStyle('white').setOpacity(0.5)
    const key = label("key"), m = margin(3)
    const h1 = hashfn(hashcode_fig, "hashCode()").setW(100).setH(30)
    const h2 = hashfn(hashcode_fig, "h(m)").setH(30)
    equal(key.x(), h1.x())
    equal(key.y0(), margin().y0())
    align("abut", "abut", h1, box().setW(10).setH(25), h2)
    const arr = small_words(hashcode_fig, 7, true)
    vertLine(arr[0].ur().toRight(7),
        arr[arr.length-1].lr().toRight(7)).setEndArrow("curved").setStartArrow("curved")
        .addLabel('m', 0.5, 7, 2)

    let g = group(arr).align("center", "abut")

    const hm_frame = rectangle().setCornerRadius(6).setFillStyle(null).setLineWidth(2)
    pin(hm_frame, hm_bkgd)
    sameSize(hm_frame, hm_bkgd)
    hm_frame.addText("HashMap").setVerticalAlign("top")
    equal(hm_frame.x0(), plus(h2.x0(), -10), plus(h1.x1(), 10))
    equal(hm_frame.y1(), m.y1())
    equal(hm_frame.y0(), m.y0())
    equal(hm_frame.x1(), m.x1())
    equal(g.x1(), minus(hm_frame.x1(), 22))

    // arrows
    saveStyle()
        setLineDash([4,4])
        arrow(key, h1)
        const p4 = point(h1.x(), plus(h1.y1(), 5))
        const p5 = point(h2.x(), p4.y())
        arrow(h1, p4, p4, p5, p5, h2)
        const p6 = point(h2.x(), plus(h2.y1(), 10))
        const p7 = point(variable("x7"), p6.y())
        const p8 = point(p7.x(), arr[4].y())
        arrow(h2, p6, p6, p7, p7, p8, p8, arr[4].cl())
        align("distribute", "none", h2.lc(), p7, arr[4].cl())
    restoreStyle()

    label("int").at(p4.toBottom(10).toRight(30))

    saveStyle()
    const note_color = '#468'
      setStrokeStyle(note_color)
      setTextStyle(note_color)
      setFontStyle("italic")
    const note1 = textFrame().addText("provided by client")
        .setW(70).setH(40).setVerticalAlign("top").setJustification("left")


    align("left", "none", note1, m)
    align("none", "abut", h1, vspace(20), note1)
    line(note1.ul().toRight(10), h1.center().toBottom(15).toLeft(15)).setEndArrow("bullet")

    const note2 = textFrame().setW(100).setH(40).addText("provided by implementer")
        .setVerticalAlign("top").setJustification("left").setInset(2)
    align("left", "bottom", note2.expand(2), hm_frame)
    line(note2.ul().toRight(10), h2.center().toBottom(10)).setEndArrow("bullet")
    restoreStyle()
}
</script>
<p class=caption>Hashing as a composition of hash functions</p>
</div>
<p>
All three of these hash table implementations rely on objects
having a method <code>hashCode()</code> that computes
the hash of an object as an <code>int</code> value. In fact,
all Java objects have such a method, inheriting it from
the base implementation <code>Object.equals()</code> unless
overridden.
</p>
<p>
The
<code>hashCode()</code> method does not know the size of the
bucket array; instead, it generates a hash that is the same regardless
of the number of buckets.
It generates the <em>input</em> to an internal hash function,
denoted as <code>h(m)</code>, which is provided by the hash table.
The internal hash function depends on the number of buckets <code>m</code>,
whereas <code>hashCode()</code> does not.
Therefore, the actual hash function used by the hash
table is the <em>composition</em> of the two methods:
<code>h(m)</code> ○ <code>hashCode</code>.
</p>
<p>
In this arrangement, the job of <code>hashCode()</code> is to hash a key object
to the full domain of the internal hash function (<code>int</code>) rather than just
to an integer in \([0,m)\). The job of the internal hash function is to convert an
arbitary <code>int</code> into an integer in \([0,m)\).
As long as at least one of these two methods provides adequate diffusion, their
composition will too.
In the current
version of the Java collection classes,
the internal hash function does not produce much diffusion, so it is important
for the client-provided <code>hashCode()</code> to do so.
</p>
<h2>Generating hash codes</h2>
<p>
A poorly designed <code>hashCode()</code> method can cause the hash table
implementation to fail to work correctly or to exhibit poor performance. There are
two main considerations:
</p>
<ol>
<li><p>For the hash table to work, the <code>hashCode()</code> method must be
consistent with the <code>equals()</code> method, because <code>equals()</code>
is used by the hash table to determine when it has found the right element or key.
Otherwise the hash table might look in the wrong bucket.
</p>

<p>In fact, it is a general requirement for Java types that if two objects
are equal according to the method <code>equals()</code>, then the values of <code>hashCode()</code>
must be equal too.
Hence, if you ever override the <code>equals()</code> method,
you must correspondingly update <code>hashCode()</code>.
</p>
</li>
<li>
<p>
The <code>hashCode()</code> function should also be collision-resistant.  Part
of avoiding collisions is that it should be as injective as possible. That is,
hashing two unequal keys should be unlikely to produce the same hash code.
This goal implies that the hash code should be computed using <em>all</em>
information in the object that affects its equality to other objects. If some
information that distinguishes two objects does not affect the hash code,
objects that differ only with respect to that ignored information will always
collide.
</p></li>
</ol>
<p>
Note that the base versions of the methods <code>equals()</code> and
<code>hashCode()</code> are guaranteed to satisfy this invariant.
Exactly how <code>Object.equals()</code> works depends on the version of Java
you are using. In older versions of Java the result of <code>hashCode()</code>
was based on the object's address in memory, but in newer versions the hash
code of an object is generated freshly when first requested, and then stored
in the object for later use.
</p>
<p>
For mutable objects, this implementation satisfies the two conditions above.
The base <code>equals()</code> method only returns true if two objects are
exactly the same object, and therefore <code>hashCode()</code> also returns the
same value.  It is usually the right choice, because two mutable objects are
only really equal if they are the same object. On the other hand, immutable
objects such as <code>String</code>s and <code>Integer</code>s have a notion of
equality that ignores the object's memory address, so these classes override
<code>hashCode()</code>.
</p>

<h3>Integer hash functions</h3>

<p>
A useful building block for hashing is an <em>integer hash function</em> that
maps from <code>int</code> to <code>int</code> or from <code>long</code> to
<code>long</code> while introducing diffusion. Two standard approaches are
<strong>modular hashing</strong> and <strong>multiplicative hashing</strong>. More expensive
techniques like cyclic redundancy checks (CRCs) and message digests are also
used in practice.
</p>

<h4>Modular hashing</h4>

<p>
With <strong>modular hashing</strong>, the hash function is simply \(h(k) = k \mod m\)
for some modulus \(m\), which is typically the number of buckets. This hash
function is easy to compute quickly when we have an integer hash code.
Some values of \(m\) tend to produce poor results, though; in particular, if
m is a power of two (that is, \(m=2^{j}\) for some \(j\)), then \(h(k)\) is
just the \(j\) lowest-order bits of \(k\):
the function \(h(k)\) would discard all the higher-order bits!
Throwing away these bits
works particularly poorly when the hash code of an object is its memory
address, as is the case for Java. At least two of the lower-order bits of an
object address will be zero, with the result that most buckets are not
used! Even if we discard these bits, there is often enough of a pattern to
object memory addresses that they will cause collisions with modular hashing.
A simple way to address this problem is to choose \(m\)
to be one <em>less</em> than a power of two. In practice, primes
work well as moduli, since computations typically will not generate keys with
a periodicity matching a large prime number. The implementation can choose
sizes from a predefined table of appropriately sized primes.
</p>

<h4>Multiplicative hashing</h4>
<p>
An alternative to modular hashing is <strong>multiplicative hashing</strong>, which is defined as
\(\newcommand\ffrac{\mathit{frac}}\)
\(h(k) = \lfloor m * \ffrac(k·A)\rfloor\), where \(A\) is a constant between 0 and 1
(e.g., Knuth recommends \(A = φ&minus;1 = 0.618033...\)), and the function \(\ffrac\)
gives the fractional part of a number: \(\ffrac(x) = x - &lfloor;x&rfloor;\).
Thus, the formula for \(h(k)\) uses the fractional part of the product \(k·A\)
to choose the bucket.
</p>
<p>
However, the formula above is not the best way to evaluate this hash function.
If \(m\) is some power of two (that is, \(m = 2^{j}\)), we can scale up the
multiplier \(A\) by \(2^{32}\) to obtain a 32-bit integer.
For example, Knuth's multiplier becomes \(A = 2^{32}·(φ-1) &equiv; &minus;1640531527 \mod 2^{32}\).
We can then evaluate the hash function cheaply as follows,
obtaining a \(j\)-bit result in \([0,m)\):
</p>
<pre>
h(k) = (k*A) &gt;&gt;&gt; (32&minus;j)
</pre>
<p>The reason this works is that the multiplication deliberately overflows, but
when integers are multiplied, only the low 32 bits of the result are retained. We
only want bits from the low 32 bits of the product anyway.
</p>
<div class=floatfigure><canvas id="multiplication" style="width: 200px; height: 200px"></canvas></div>
<script class=graphics>
with (new CFigure("multiplication")) {
    function word() {
        return rectangle().setW(80).setH(12)
    }
    let w1 = word(),
        w2 = word().addText("A"),
        w = w1.w(), h = 5
    label("A").setFontSize(9).at(w1)
    label("k").setFontSize(9).at(w2)
    align("none", "top", w1, canvasRect().inset(2))
    align("left", "none", w1, w2)
    equal(w2.y0(), plus(h, w1.y1()))
    let bar = horzLine("black", 2), bar2 = horzLine("black", 2)
    equal(bar.y(), plus(w2.y1(), h))
    equal(bar.x1(), w1.x1())
    equal(bar.w(), plus(w, 30))
    equal(bar.x1(), minus(canvasRect().x1(), 5))
    let x = label("×")
    align("left", "abut", x, vspace().setH(h), bar)
    let wi = [], wp = bar
    for (let i = 0; i < 8; i++) {
        wi[i] = word()
        equal(wi[i].y0(), plus(wp.y1(), h))
        equal(wi[i].x1(), minus(w1.x1(), times(w, i/8)))
        wp = wi[i]
    }
    align("none", "abut", wp, vspace().setH(h), bar2)
    align("right", "none", bar2, w2)
    let sum = rectangle().setH(w1.h())
    equal(sum.w(), times(2, w1.w()))
    align("right", "abut", bar2, vspace().setH(h), sum)
    align("left", "none", bar2, sum)
    let ll = vertLine(), lr = vertLine()
    equal(ll.x(), w2.x0())
    equal(ll.y0(), bar.y(), lr.y0())
    equal(ll.y1(), sum.y1(), lr.y1())
    equal(lr.x(), wi[5].x1())
    label("j").setX(average(lr.x(), ll.x())).setY(sum.y()).setFontSize(9)
}
</script>
<p>
Implemented properly, multiplicative hashing is faster and offers better diffusion
than modular hashing. Intuitively, multiplying together two
large numbers diffuses information from each of them into the product,
especially around the middle bits of the product. The formula above picks out
<code>j</code> bits from the middle of the product \(k·A\).
If we visualize what happens when we multiply two 32-bit numbers, we can see that
every bit of <code>k</code> can potentially affect those <code>j</code> bits,
as shown on the right.
</p>

<p>Unfortunately, multiplicative hashing is often implemented incorrectly
and has unfairly acquired a bad reputation in some quarters because of it.
A common mistake is to implement it as
\((k·A) \mod m\). By the properties of modular arithmetic,
\((k·A) \mod m = ((k \mod m) × (A \mod m) \mod m)\). Therefore, this mistaken implementation
acts just like modular hashing in combination with
shuffling the order of the buckets; it adds no diffusion.
</p>
<p>
With multiplicative hashing, diffusion into the lower-order bits is not as good as
into the higher-order bits, because fewer bits of either the multiplier or the hashcode
affect them. This effect can increase clustering for large hash tables where lower-order
bits affect the choice of bucket.
</p>
<p>
An easy way to increase diffusion in the lower bits is to use 64-bit
multiplication instead:
</p>
<pre>
h(k) = (A * (long)k) &gt;&gt;&gt; (64&minus;j)
</pre>
<p>
where <code>A</code> is now a 64-bit representation of the multiplier.
</p>

<h3>Adversarial computation and cryptographic hash functions</h3>
<p>
The goal of the hash table is that collisions should
occur as if at random. Consequently, whether collisions occur depends to some
extent on the keys generated by the client. If the client is an adversary
trying to produce collisions, the hash table must work harder. Early web
sites implemented using the Perl programming language were subject to
denial-of-service attacks that exploited the ability to engineer hash table
collisions. Attackers used their knowledge of Perl's hash function on strings
to craft strings (e.g., usernames) that collided, effectively turning Perl's hash tables
into linked lists.
</p>
<p>
To produce hashes resistant to an adversary, a <strong>cryptographic hash function</strong>
should be used. The message digest algorithms MD5, SHA-1,
and SHA-2 are good choices whose security increases (and performance
decreases) in that order. They are available in Java through the class
<tt>java.security.MessageDigest</tt>. Viewing the data to be hashed as
a string or byte array s, the value <code>MD5(IV + s) mod m</code> is a
cryptographic hash function offering a good balance between security and
performance. MD5 generates 128 bits of output, so if \(m = 2^{j}\),
the hash function can simply pick \(j\) bits from the MD5 output.
If \(m\) is not a power of two, modular hashing can be
applied to the message digest.
</p>
<p>
The value
<code>IV</code> is the <strong>initialization vector</strong>. It should be randomly generated
when the program starts, using a high-entropy input source such as the
class <code>java.security.SecureRandom</code>. The initialization vector
prevents the adversary from testing possible values of <code>s</code> ahead of
time. For very long-running programs, it is also prudent to
refresh <code>IV</code> periodically, to guard against an adversary
who is trying to learn the hash function.
However, note that refreshing IV requires rehashing all hash tables that depend
on it.
</p>

<h3>Hashing immutable data structures</h3>
<p>
Immutable data abstractions such as <code>String</code> would normally define
their notion of equality in terms of the data they contain rather than their
memory address. The <code>hashCode()</code> method should therefore return
the same hashcode for objects that represent the same data value. For large
objects, it is important in general to use all of the information that makes
up the value; otherwise collisions can easily result. Fortunately, a
hash function that operates on fixed-sized blocks of information can be
used to construct a hash function that operates on an arbitrary amount of data.
Suppose the immutable object can be uniquely encoded as a finite sequence of integer
data values \(d_{1}, d_{2}, ..., d_n\), and we have an
integer hash function \(h\) that maps any integer value into a integer
hash value with good diffusion. In that case, a good hash function \(H\) can be constructed iteratively
by feeding the output of \(H\) on all previous values to \(h\).
Using \(H_{i}\) to represent the hash of the first \(i\)
values in the sequence, and &oplus; to represent the bitwise <em>exclusive or</em> operator, we have:
</p>

<blockquote>
\[
    H_{1} = h(d_{1})
\]
\[
    H_{i+1} =
        h(H_{i} ⊕ d_{i+1})
\]
\[
    H(d_{1}, ..., d_n) = H_{n}
\]
</blockquote>

<p>Diagrammatically, the computation looks as follows:</p>
<div class=figure>
<canvas id="hashdigest" style="width: 400px; height: 100px"></canvas>
<script class=graphics>
with (new CFigure("hashdigest")) {
    function dv(n) {
        return rectangle().addText(italic("d"), subscript("" + n)).setH(18).setW(50)
    }
    function h() {
        return rectangle().addText(italic("h"))
                          .setH(18).setW(50).setLineWidth(2)
                          .setFillStyle("#dff")
    }
    function oplus() {
        let c = circle().setW(15).setLineWidth(2)
        align("center", "top bottom", vertLine("black", 2), c)
        align("left right", "center", horzLine("black", 2), c)
        return c
    }
    let d1, d2, dn
    align("distribute", "center", d1 = dv(1), d2 = dv(2), label("..."), dn = dv("n"))
    equal(d1.x0(), 2)
    equal(d1.y0(), 2)
    equal(dn.x1(), minus(canvasRect().x1(),80))
    let h1, h2, h3
    align("none", "center", h1 = h(), h2 = h(), h3 = h())
    equal(h1.y0(), plus(d1.y1(), 50))
    equal(h1.x0(), d1.x1())
    equal(h2.x0(), d2.x1())
    equal(h3.x0(), dn.x1())
    arrow(d1.lc(), h1.cl().toLeft(30), h1.cl())
    let xor1 = oplus(), xor2 = oplus(), dots = label("...").setFontSize(18)
    align("center", "none", xor1, d2)
    align("center", "none", xor2, dn)
    align("none", "center", xor1, xor2, h1, dots)
    arrow(d2.lc(), xor1.uc())
    arrow(h1.cr(), xor1.cl())
    arrow(xor1.cr(), h2.cl())
    arrow(h2.cr(), dots)
    equal(dots.x0(), plus(h2.x1(), 30))
    arrow(dots.cr(), xor2.cl())
    arrow(dn.lc(), xor2.uc())
    arrow(xor2.cr(), h3.cl())
    arrow(h3.cr(), h3.cr().toRight(30))
}
</script>
</div>

<p>
Note that this is very different from the alternative of
hashing all the individual data values and combining them with exclusive-or.
That perhaps tempting algorithm would tend to have more collisions because
the commutativity and associativity of exclusive-or would cause that algorithm
to give the same result on any permutation of the data values.
</p>
<p>
To make hash values unpredictable across runs, it is desirable to
compute the first hash value in the sequence by combining it with
an initialization vector: \( H_1 = h(d_1 ⊕ IV) \), where
the initialization vector \(IV\) is chosen at the start
of the application.
</p>
<p>
A related way to generate hash codes is to think of them as involving
the evaluation of a polynomial whose coefficients are the data going into
the hash code. It is a property of polynomials that there is a vanishing
likelihood that two unequal polynomials take on the same value when evaluated
at a randomly chosen real number. For hashing we are not dealing with real numbers,
but we can evaluate polynomials at a “random” large integer \(x\) to much the same
effect, as long as the integer is relatively prime to the number of buckets.
This evaluation looks like the diagram above, except that the function \(h\)
is implemented as multiplication by \(x\), and addition is substituted for
exclusive or. (In fact, the polynomial can be evaluated using exclusive or
as well.) For example, if we wanted to hash all the values in an array of integers,
code like the following efficiently performs the necessary polynomial
evaluation (mod \(2^{32}\)), assuming an appropriate constant \(x\) and
initialization vector <code>IV</code>:
</p>
<pre>
int[] data;
int result = IV;
for (int i = 0; i < data.length; i++) {
    result = result * x + data[i];
}
return result;
</pre>

<h3>Java hashing tips</h3>
<p>
Methods for combining data to construct a hash are already
provided by existing Java libraries.  One trick for constructing a good
<code>hashCode()</code> is to lean on the <code>hashCode()</code> method
of strings.  If your class defines a <code>toString()</code> method in such a
way that it returns the same strings when two objects are equal, then
<code>hashCode()</code> can simply return the hash of that string!
</p>
<pre>
  // Requires: o1.toString().equals(o2.toString()) if o1.equals(o2)
  int hashCode() {
    return toString().hashCode();
  }
</pre>
<p>However, note that if two unequal objects have the same
<code>toString()</code> output, they will <em>always</em> collide in the hash
table. For the best
performance, <code>toString()</code> should return the same string
<em>exactly</em> when the two objects are equal.
</p>
<p>
Converting to a string is simple but often adds unnecessary overhead; for
better performance, Java offers two standard methods for constructing
high-quality hash codes from a sequence of data.
</p>

<ul>
<li><code>Arrays.hashCode(Object[])</code>: Given an array containing a
sequence of objects, this method combines their hash codes into a
single hash code. For example, suppose we have a class
<code>Point3D</code> representing an immutable three-dimensional point with
fields <code>x</code>, <code>y</code>, and <code>z</code> for its coordinates.
We might implement the <code>hashCode</code> method of the class as follows:
<pre>
import java.util.Arrays;
class Point3D {
    private int x, y, z;
    public int hashCode() {
        return Arrays.hashCode(new int[]{x, y, z});
    }
}
</pre>
<p>
The current implementation of <code>Arrays.hashCode</code> produces a hash of
only moderate quality.  It uses the polynomial technique above, in which the
individual hash codes are treated as coefficients of a polynomial evaluated at
31. This approach is good enough for many applications.
</p>
<li><code>Objects.hash(Object... values)</code>: This method takes
a variable-length argument list and computes a hash code from its
elements in the same way that <code>Arrays.hashCode</code> does on an
array containing those elements. Thus, ther <code>Point3D</code> hash code could
be implemented as follows:
<pre>
import java.util.Objects;
class Point3D {
    private int x, y, z;
    public int hashCode() {
        return Objects.hash(x, y, z);
    }
}
</pre>
</ul>

<p>One Java pitfall to watch out for arises because Java's collection classes
override <code>hashCode()</code> to compute the hash from the current
contents of the collection, as if Java collections were immutable.  This way of
computing the hash code is dangerous, precisely because collections are
<em>not</em> immutable.  Mutating a collection used as a key will change
its hash, breaking the class invariant of the hash table. Any collection
being used as a key must not be mutated.
</p>

<h2>More performance tips</h2>
<h3>Precomputing hash codes</h3>
<p>
High-quality hash functions can be expensive. If the same values are
being hashed repeatedly, one trick is to precompute their hash codes
and store them with the value. Hash tables can also store the full hash
codes of values, which speeds up scanning a bucket: there is no
need to do a full equality test on the keys if their hash codes don't
match. In fact, if the hash code is long and the hash function is
cryptographically strong (e.g., 64+ bits of a properly constructed MD5
digest), two keys with the same hash code are almost certainly the same
value. Your computer is then more likely to get a wrong answer from a
cosmic ray hitting it than from a collision in random 64-bit data.
</p>
<p>
Precomputing and storing hash codes is an example of a <strong>space-time tradeoff</strong>,
in which we speed up computation at the cost of using
extra memory.
</p>

<h3>Detecting clustering</h3>
<p>
When the distribution of keys into buckets is not random, we say that the hash
table exhibits <strong>clustering</strong>. If you care about performance,
it is a good idea to test your hash
function to make sure it does not exhibit clustering. With any
hash function, it is possible to generate data that cause it to behave poorly,
but a good hash function will make this unlikely.
A good way to determine whether a hash function is working well is to check for
clustering. 
</p>
<p>
Unfortunately, most hash table implementations, including those in the Java
Collections Framework, do not give the client a way to check for clustering.
Clients can't easily tell whether the hash function is performing well.
We can hope that it will be standard practice for future hash table designers
to provide clustering estimation as part of the interface.
</p>
<h4>Counting empty buckets</h4>
<p>
One simple way to check for clustering is to measure the fraction of
empty buckets. With a uniform hash function, the probability that a given
bucket is empty is \( (1 - 1/m)^n ≈ (1/e)^α \). If the fraction of empty buckets
is higher than that, it indicates the hash function is doing worse than random.
If lower, it is better than random, which can happen when the data being hashed
has structure that interacts in a felicitous way with the hash function being used.
For example, if the keys are sequential integers, and the hash function
is just “mod \(m\)”, the keys will be distributed as evenly as possible among
the buckets, more evenly than if they were mapped randomly onto buckets.
In general, you should not count on this happening!
</p>
<h4>Using bucket size variance</h4>
<p>
A more informative way to measure clustering looks at the sizes of the buckets.
It is based on the <strong>variance</strong> of the distribution of bucket
sizes. If clustering is occurring, some buckets will have more elements than
they should, and some will have fewer. Bucket
sizes will vary more than one would expect from a random hash function.
</p>
<p>
The average size of a bucket is the load factor \(α\). A given
if bucket \(i\) contains \(x_i\) elements, which deviates from
the expected size by \(x_i - α\). Clustering can be
measured by computing the sum of the squares of these deviations:
\(\sum \left({x_i} - α\right)^2\). This sum can then be
compared to its expected value. If the hash function satisfies the uniform
hashing assumption and \(M\) buckets are summed over, this expected value is:
\[
    Mα(1 - 1/m)
\]
</p>
<p>
Therefore, a reasonable way to measure clustering \(C\) is as the ratio of the
sum of square deviations taken over \(M\) buckets and this quantity:
\[
C = {
        \sum^M {(x_i - α)}^2
        \over
        {Mα(1 - 1/m)}
    }
\]

<p>
Appealingly, this measure is not affected much by the load factor of the hash table.
A uniform hash function produces clustering \(C\) near 1.0 with high probability when
\(n\) is large.
A clustering measure \(C\) that is
greater than one means that clustering slows down the performance of the hash table by
approximately a factor of \(C\).
For example, if \(m = n\) and all elements are hashed into one bucket, the
clustering measure is approximately \(n\). If the hash function is
perfect and every element lands in its own bucket, the clustering measure
will be 0. If the clustering measure is less than 1.0, the hash
function is spreading elements out more evenly than a random hash function
would.
</p>
